<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Shivram S ai24btech11031@iith.ac.in" />
  <title>AI1001 - Assignment 13</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">AI1001 - Assignment 13</h1>
<p class="author">Shivram S<br />
<code>ai24btech11031@iith.ac.in</code></p>
</header>
<h1 id="convolutional-networks">Convolutional Networks</h1>
<p>Inputs such as images can‚Äôt be thought of as a simple vector of input
pixel values because the adjacency of pixels matters. If we had a
network with fully connected layers, the semantics of adjacency would be
lost. Furthermore, such a network would have a large number of weights,
which would require vast numbers of training images, and a huge
computational budget.</p>
<p>We can construct the first hidden layer so that each unit receives
input only from a small, local region of the image. This helps preserve
<strong>adjacency</strong> and cuts down on the number of weights. By
using the same weights for each hidden unit, we can achieve
<strong>spatial invariance</strong>. Each hidden unit becomes a
<strong>feature detector</strong> that detects the same feature wherever
it appears in the image. We can use multiple hidden units with multiple
distinct sets of weights to detect multiple features.</p>
<p>A <strong>convolutional neural network</strong> (CNN) is one that
contains spatially local connections, and has patterns of weights
(<strong>kernels</strong>) replicated across all units in a later. The
process of applying the kernel to the image is called
<strong>convolution</strong>. We can define the convolution,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">ùê≥</mtext><mo>=</mo><mtext mathvariant="bold">ùê±</mtext><mo>*</mo><mtext mathvariant="bold">ùê§</mtext></mrow><annotation encoding="application/x-tex">\textbf{z} = \textbf{x} * \textbf{k}</annotation></semantics></math>,
of the input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ùê±</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
and the kernel
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ùê§</mtext><annotation encoding="application/x-tex">\textbf{k}</annotation></semantics></math>
of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>,
as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><msub><mi>k</mi><mi>j</mi></msub><msub><mi>x</mi><mrow><mi>j</mi><mo>+</mo><mi>i</mi><mo>‚àí</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_i = \sum_{j=1}^{l} k_j x_{j + i - (l+1)/2}</annotation></semantics></math></p>
<p>Instead of applying the kernel to adjacent groups of pixels, we might
increase the separation between groups, called the
<strong>stride</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>.
A larger stride reduces the number of pixels in the output layer. For
smaller kernels, we typically use
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">s = 1</annotation></semantics></math>.
We might also add some <strong>padding</strong> to our input, so that
the kernel can also be applied to the values at the edges.</p>
<p>If we try to detect
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>
different featues using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>
different kernels with a stride of 1, the output will be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>
times larger than the input. A two-dimensional input array will become a
three-dimensional array of hidden units, where the third dimension is of
size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>.
This additional ‚Äúkernel dimension‚Äù does not have any adjacency
properties.</p>
<h2 id="pooling">Pooling</h2>
<p>A <strong>pooling</strong> layer in a neural network summarizes a set
of adjacent units from the preceding layer with a single value. Pooling
works just like a convolution layer, but the operation that is applied
is fixed rather than learned. There are two common forms of pooling:</p>
<ul>
<li><p><strong>Average-pooling</strong> computes the average value of
its
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
inputs. This is identical to convolution with a uniform kernel
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">ùê§</mtext><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>1</mn><mi>/</mi><mi>l</mi><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mn>1</mn><mi>/</mi><mi>l</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\textbf{k} = [1/l, \dots, 1/l]</annotation></semantics></math>.
If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>=</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">l = s</annotation></semantics></math>,
the layer <strong>downsamples</strong> the image by a factor of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>.</p></li>
<li><p><strong>Max-pooling</strong> computes the maximum value of its
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
inputs. It can be used for downsampling, but the semantics are
different. Generally, max-pooling acts as a kind of logical disjunction,
saying that a feature exists somewhere in the unit‚Äôs receptive
field.</p></li>
</ul>
<h2 id="tensor-operations">Tensor Operations</h2>
<p><strong>Tensors</strong> are multidimensional arrays of any
dimension. They are generalizations of vectors and matrices. Tensors
help CNNs keep track of the ‚Äúshape‚Äù of the data. Tensors are also
computationally efficient. Software can generate highly optimized code
for tensor operations which are often run on GPUs (graphics processing
units) or TPUs (tensor processing units).</p>
<p>For example, if we train on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn><mo>√ó</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">256 \times 256</annotation></semantics></math>
RGB images with a minibatch size of 64, we will have an input tensor of
size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn><mo>√ó</mo><mn>256</mn><mo>√ó</mo><mn>3</mn><mo>√ó</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">256 \times 256 \times 3 \times 64</annotation></semantics></math>.
Applying 96 kernels of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>√ó</mo><mn>5</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">5 \times 5 \times 3</annotation></semantics></math>
with a stride
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>x</mi></msub><mo>=</mo><msub><mi>s</mi><mi>y</mi></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">s_x = s_y = 2</annotation></semantics></math>
gives us an output tensor of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn><mo>√ó</mo><mn>128</mn><mo>√ó</mo><mn>96</mn><mo>√ó</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">128 \times 128 \times 96 \times 64</annotation></semantics></math>.
Such a tensor is often called a <strong>feature map</strong>. In this
case, it is composed of 96 <strong>channels</strong>, where each channel
carries information from one feature.</p>
<h1 id="residual-networks">Residual Networks</h1>
<p><strong>Residual networks</strong> are a popular approach to building
very deep networks that avoid the problem of vanishing gradients.</p>
<p>Typically layers in deep models completely replace the representation
at the previous layer. Because each layer replaces the representation
from the preceding layer, all the layers must learn to do something
useful. If we set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">ùêñ</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="bold">ùüé</mtext></mrow><annotation encoding="application/x-tex">\textbf{W}^{(i)} = \textbf{0}</annotation></semantics></math>
for any layer, the entire network would cease to function. If we also
set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">ùêñ</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>‚àí</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="bold">ùüé</mtext></mrow><annotation encoding="application/x-tex">\textbf{W}^{(i-i)} = \textbf{0}</annotation></semantics></math>,
the network would be unable to learn.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">ùê≥</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ùê≥</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mtext mathvariant="bold">ùê†</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ùêñ</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mtext mathvariant="bold">ùê≥</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textbf{z}^{(i)} = f(\textbf{z}^{(i-1)}) = \textbf{g}^{(i)} (\textbf{W}^{(i)} \textbf{z}^{(i-1)})</annotation></semantics></math></p>
<p>The key idea of residual networks is that a layer should
<span><em>perturb</em></span> the representation from the previous layer
rather than <span><em>replace</em></span> it entirely.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">ùê≥</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msubsup><mtext mathvariant="bold">ùê†</mtext><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ùê≥</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mtext mathvariant="bold">ùêñ</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mtext mathvariant="bold">ùê≥</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textbf{z}^{(i)} = \textbf{g}_r^{(i)} (\textbf{z}^{(i-1)} + \textbf{W}^{(i)} \textbf{z}^{(i-1)})</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ùê†</mtext><annotation encoding="application/x-tex">\textbf{g}</annotation></semantics></math>
is the activation function for the residual layer, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
is the <strong>residual</strong>, usually defined as a neural network
with one nonlinear layer combined with one linear layer:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ùê≥</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="bold">ùêï</mtext><mtext mathvariant="bold">ùê†</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ùêñ</mtext><mtext mathvariant="bold">ùê≥</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\textbf{z}) = \textbf{V} \textbf{g} ( \textbf{W} \textbf{z})</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ùêñ</mtext><annotation encoding="application/x-tex">\textbf{W}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ùêï</mtext><annotation encoding="application/x-tex">\textbf{V}</annotation></semantics></math>
are learned matrices. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">ùêï</mtext><mo>=</mo><mtext mathvariant="bold">ùüé</mtext></mrow><annotation encoding="application/x-tex">\textbf{V} = \textbf{0}</annotation></semantics></math>,
then the layer passes its inputs through with no change. Whereas
traditional networks must learn to propagate information, residual
networks propagate information by default.</p>
<h1 id="learning-algorithms">Learning Algorithms</h1>
<p>To train a neural network, any kind of optimization algorithm could
be used, but in practice, modern neural networks are almost always
trained with some variant of stochastic gradient descent. A few
important considerations when training neural networks are:</p>
<ul>
<li><p>The dimensionality of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ùê∞</mtext><annotation encoding="application/x-tex">\textbf{w}</annotation></semantics></math>
and the size of the training set are very large. Using SGD with a
relatively small minibatch size allows helps the algorithm escape small
local minima through stochasticity. The small minibatch size ensures
that the computational cost of each step is a small constant.</p></li>
<li><p>Gradient contributions of each example can be computed
independently, which lets us take advantage of parallelism in GPUs or
TPUs.</p></li>
<li><p>To improve convergence, we choose a learning rate that decreases
over time.</p></li>
<li><p>Gradients from small minibatches may have high variance, and the
gradient may point in the wrong direction, making convergence difficult.
This may be solved by increasing minibatch size as training proceeds.
Another approach is to incorporate the idea of
<strong>momentum</strong>, which keeps a running average of the
gradients of past minibatches.</p></li>
<li><p>Care must be taken to mitigate numerical instabilities such as
overflow, underflow, and rounding error.</p></li>
</ul>
<p>Back-propagation can be used for any feedforward computation graph.
The back-propagation process passes messages back along each link in the
network.</p>
<p>Suppose we have a computation graph where nodes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
are inputs to node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>,
which itself is an input to nodes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>.
We know that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
affects the output through
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>,
so we can compute the derivative of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>
with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
by summing incoming messages from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>‚àÇ</mi><mi>L</mi></mrow><mrow><mi>‚àÇ</mi><mi>h</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>‚àÇ</mi><mi>L</mi></mrow><mrow><mi>‚àÇ</mi><msub><mi>h</mi><mi>j</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>‚àÇ</mi><mi>L</mi></mrow><mrow><mi>‚àÇ</mi><msub><mi>h</mi><mi>k</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial h} = \frac{\partial L}{\partial h_j} + \frac{\partial L}{\partial h_k}</annotation></semantics></math></p>
<p>Using this, we can compute the outgoing messages to its input
nodes</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>‚àÇ</mi><mi>L</mi></mrow><mrow><mi>‚àÇ</mi><msub><mi>f</mi><mi>h</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>‚àÇ</mi><mi>L</mi></mrow><mrow><mi>‚àÇ</mi><mi>h</mi></mrow></mfrac><mfrac><mrow><mi>‚àÇ</mi><mi>h</mi></mrow><mrow><mi>‚àÇ</mi><msub><mi>f</mi><mi>h</mi></msub></mrow></mfrac><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><mfrac><mrow><mi>‚àÇ</mi><mi>L</mi></mrow><mrow><mi>‚àÇ</mi><msub><mi>g</mi><mi>h</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>‚àÇ</mi><mi>L</mi></mrow><mrow><mi>‚àÇ</mi><mi>h</mi></mrow></mfrac><mfrac><mrow><mi>‚àÇ</mi><mi>h</mi></mrow><mrow><mi>‚àÇ</mi><msub><mi>g</mi><mi>h</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial f_h} = \frac{\partial L}{\partial h} \frac{\partial h}{\partial f_h} \text{ and }
\frac{\partial L}{\partial g_h} = \frac{\partial L}{\partial h} \frac{\partial h}{\partial g_h}</annotation></semantics></math></p>
<p>The initial messages for back-propagation are generated by
calculating the derivative of the loss function. These are used by nodes
to compute the gradients for each weight. The computational complexity
scales linearly with the number of nodes. Despite requiring all the
intermediate values to be stored and increasing the memory requirement
for training a model, this can greatly simplify and speed up
training.</p>
<h2 id="batch-normalization">Batch Normalization</h2>
<p>We can improve the rate of convergence of SGD by rescaling the values
generated at the internal layers of the network from the example within
each minibatch. Though the exact reason is not understood, it seems to
have effects similar to the residual network.</p>
<p>Consider a node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>
in the network. The values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>
for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
examples are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>z</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">z_1, \dots, z_m</annotation></semantics></math>.
Batch normalization replaces each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math>
with</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>z</mi><mo accent="true">ÃÇ</mo></mover><mi>i</mi></msub><mo>=</mo><mi>Œ≥</mi><mfrac><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>‚àí</mo><mi>Œº</mi></mrow><msqrt><mrow><mi>œµ</mi><mo>+</mo><msup><mi>œÉ</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mo>+</mo><mi>Œ≤</mi></mrow><annotation encoding="application/x-tex">\hat z_i = \gamma \frac{z_i - \mu}{\sqrt{\epsilon + \sigma^2}} + \beta</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œº</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
is the mean value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÉ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
is the standard deviation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>
is a small constant added to prevent division by zero.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
are learned parameters which may be node-specific or layer-specific.</p>
<p>Without batch normalization, information can get lost if a layer‚Äôs
weights are too small. Batch normalization prevents this from happening
by standardizing the mean and variance of the values.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
are included in the training process. After training, they are fixed at
the learned values.</p>
<h1 id="generalization">Generalization</h1>
<h2 id="choosing-a-network-architecture">Choosing a network
architecture</h2>
<p>A lot of progress in performance has come from exploring different
kinds of network architectures, and by varying the number of layers and
their connectivity. Some architectures are designed to generalize well:
convolutional networks encode the idea that the same feature extractor
is useful at all locations, and recurrent networks encode the idea that
the same update rule is useful at all points in a stream of sequential
data.</p>
<p>Deep networks perform better than other machine learning approaches
on tasks with higher dimensionality. Other approaches require
preprocessing to reduce dimensionality and achieve performance
comparable to deep learning systems.</p>
<p>However, deep learning models lack expressive power, and might
produce unintuitive errors. They produce input-output mappings that are
discontinuous, so a small change in input might cause a large change in
the output. Such <strong>adversarial examples</strong> are easier to
find in higher dimensions. They suggest that deep learning models
recognize objects in ways that are very different from the human
brain.</p>
<p>There are no clear sets of guidelines to choose the best network
architecture for a problem. It is common to use <strong>neural
architecture search</strong> to explore the space of possible
architectures. <strong>Evolutionary algorithms</strong> have been
popular because it is sensible to do recombination and mutation.</p>
<p>Estimating the value of a candidate network is a major challenge. We
can evaluate an architecture by training it and evaluating its accuracy,
but with large networks this is computationally expensive.</p>
<p>We can speed up estimation by training on a smaller data set, or by
using a reduced number of batches and predicting the performance. We can
also train a large network and search for subgraphs that perform better.
This can be fast because the subgraphs share parameters and don‚Äôt have
to be retrained.</p>
<p>We can also learn a <strong>heuristic evaluation function</strong> by
training and testing some networks. Then we can generate a large number
of candidate networks and quickly estimate their value using this
function.</p>
<h2 id="weight-decay">Weight decay</h2>
<p><strong>Weight decay</strong> is a regularization method in which a
penalty
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi><msub><mo>‚àë</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><msubsup><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\lambda \sum_{i,j} W_{i,j}^2</annotation></semantics></math>
is added to the loss function, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
is a hyperparameter controlling the strength of the penalty.</p>
<p>In networks with sigmoid activation functions, it is hypothesized
that the weight decay keeps activations near the linear part of the
sigmoid, avoiding vanishing gradients. In residual networks, it
encourages he network to have small differences between consecutive
layers.</p>
<p>One explanation for the beneficial effect of weight decay is that it
implements a form of <em>maximum a posteriori</em> (MAP) learning. The
MAP hypothesis
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>h</mi><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub><annotation encoding="application/x-tex">h_{MAP}</annotation></semantics></math>
satisfies:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>h</mi><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mo>argmax</mo><mtext mathvariant="bold">ùê∞</mtext></msub><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ùê≤</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="bold">ùêó</mtext><mo>,</mo><mtext mathvariant="bold">ùêñ</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ùêñ</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mo>argmin</mo><mtext mathvariant="bold">ùê∞</mtext></msub><mrow><mo stretchy="true" form="prefix">[</mo><mo>‚àí</mo><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ùê≤</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="bold">ùêó</mtext><mo>,</mo><mtext mathvariant="bold">ùêñ</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ùêñ</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
h_{MAP} &amp;= \mathop{\mathrm{argmax}}_{\textbf{w}} P(\textbf{y} | \textbf{X}, \textbf{W}) P(\textbf{W}) \\
 &amp;= \mathop{\mathrm{argmin}}_{\textbf{w}} \left[ - \log P(\textbf{y} | \textbf{X}, \textbf{W}) - \log P(\textbf{W}) \right]
\end{aligned}</annotation></semantics></math></p>
<p>The first term is the usual cross-entropy loss, and the second term
prefers weights under a prior distribution. We can make this align with
a regularized loss function if we set</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ùêñ</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>‚àí</mo><mi>Œª</mi><munder><mo>‚àë</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><msubsup><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\log P(\textbf{W}) = - \lambda \sum_{i,j} W_{i,j}^2</annotation></semantics></math></p>
<h2 id="dropout">Dropout</h2>
<p><strong>Dropout</strong> tries to reduce the test-set error of the
network at the cost of making it harder to fit the training set. At each
step of training, dropout deactivates a randomly chosen subset of units
and applies one step of back-propagation. This is a rough approximation
to training a large ensemble of different networks.</p>
<p>By introducing noise at training time, the model is forced to become
robust to noise. Hidden units trained with dropout must learn to be
compatible with many other possible sets of hidden units, which is
similar to the selection processes that guide evolution. Dropout applied
to later layers in a network forces the final decision to be made by
paying attention to all the abstract features.</p>
<p>By forcing the model to learn multiple robust explanations for each
input, dropout makes the model generalize well. However, to fit the
training set, it is usually necessary to use a larger model and train it
for more iterations.</p>
</body>
</html>
