<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Shivram S ai24btech11031@iith.ac.in" />
  <title>AI1001 - Assignment 8</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">AI1001 - Assignment 8</h1>
<p class="author">Shivram S<br />
<code>ai24btech11031@iith.ac.in</code></p>
</header>
<p><strong>Learning</strong> is when an agent improves its performance
after making observations about the world. When the agent is a computer,
we call it <strong>machine learning</strong> a computer observes data,
builds a model based on the data, and uses the model as a hypothesis
about the world and as a piece of software that can solve problems.</p>
<p>We want machines to learn because the designers can’t anticipate all
possible future situations, and sometimes even the designers have no
idea how to program the solution themselves.</p>
<p>Any component of an agent program can be improved by machine
learning. The improvements and techniques depend on what component is to
be improved, what prior knowledge the agent has, and what data and
feedback is available.</p>
<p>The three main types of learning are classified based on the feedback
accompanying the inputs:</p>
<ul>
<li><p>Supervised Learning: The agent observes input-output pairs and
learns a function that maps from input to output.</p></li>
<li><p>Unsupervised Leaning: The agent learns patterns in the input
without explicit feedback. This is commonly used in
<strong>clustering</strong>.</p></li>
<li><p>Reinforcement Learning: The agent learns from a series of rewards
and punishments (reinforcements). It is up to the agent to decide which
actions were responsible for the reinforcement.</p></li>
</ul>
<h1 id="supervised-learning">Supervised Learning</h1>
<p>In supervised learning, given a training set of example input-output
pairs, generated by an unknown function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>,
we have to discover a function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
that approximates the true function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>.
The function is called a <strong>hypothesis</strong> and it is drawn
from a <strong>hypothesis space</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℋ</mi><annotation encoding="application/x-tex">\mathcal H</annotation></semantics></math>
of possible functions. We say that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>
is a <strong>model</strong> of the data drawn from a <strong>model
class</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>H</mi></mrow><annotation encoding="application/x-tex">mathcal H</annotation></semantics></math>.</p>
<p>Ideally, we want a consistent hypothesis, such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h(x_i) = y_i</annotation></semantics></math>
for every
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math>
in the training set. When this is not possible, we look for a
<strong>best-fit function</strong> The true measure of a hypothesis is
how it handles inputs it has not yet seen, i.e. it
<strong>generalizes</strong> well. We say that a hypothesis is
<strong>underfitting</strong> when it fails to find a pattern in the
data, and <strong>overfitting</strong> when it performs well on the
training data but performs poorly on unseen data.</p>
<p>Sometimes, we try to find the hypothesis
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>h</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">h^\star</annotation></semantics></math>
that is the most probable given the data:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>=</mo><msub><mo>argmax</mo><mrow><mi>h</mi><mo>∈</mo><mi>H</mi></mrow></msub><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>∣</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h^* = \mathop{\mathrm{argmax}}_{h \in H} P(h \mid data)</annotation></semantics></math></p>
<p>By Bayes’ rule, this is equivalent to</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>=</mo><msub><mo>argmax</mo><mrow><mi>h</mi><mo>∈</mo><mi>H</mi></mrow></msub><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>∣</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h^* = \mathop{\mathrm{argmax}}_{h \in H} P(data \mid h) P(h)</annotation></semantics></math></p>
<p>Hypotheses spaces can be analyzed based on the <strong>bias</strong>
they impose, i.e. the tendency of the hypothesis to deviate from the
expected value over different training sets, and the
<strong>variance</strong> they produce, i.e the amount of change in the
hypothesis due to fluctuation in the training data. There is a tradeoff
between complex low-bias-high-variance hypotheses and simpler,
low-variance-high-bias hypotheses.</p>
<p>When we choose a more expressive hypothesis space
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℋ</mi><annotation encoding="application/x-tex">\mathcal H</annotation></semantics></math>,
the computational complexity of finding a good hypothesis within that
space increases. For example, fitting a straight line to data is easy
but fitting a high-degree polynomial is harder. Hence, most work on
learning has focused on simple representations. But, there has also been
work on deep learning, which uses a complex representation, but the
number of steps required to compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(x)</annotation></semantics></math>
is still bounded.</p>
<h1 id="example-restaurant-waiting">Example: Restaurant Waiting</h1>
<p>Suppose we want to solve the problem of deciding whether to wait for
a table at a restaurant. The output is a boolean variable representing
whether we will wait. The input may have several discrete attributes,
such as:</p>
<ol>
<li><p><strong>Alternate</strong> Whether there is a suitable alternate
restaurant nearby</p></li>
<li><p><strong>Bar</strong> Whether the restaurant has a comfortable
area to wait in</p></li>
<li><p><strong>Fri/Sat</strong> True on Fridays and Saturdays</p></li>
<li><p><strong>Hungry</strong> Whether we are hungry right now</p></li>
<li><p><strong>Patrons</strong> How many people are there in the
restaurant</p></li>
<li><p><strong>Price</strong> The restaurant’s price range</p></li>
<li><p><strong>Raining</strong> Whether it is raining outside</p></li>
<li><p><strong>Reservation</strong> Whether we made a
Reservation</p></li>
<li><p><strong>Type</strong> The kind of restaurant (French, Italian,
Thai, etc.)</p></li>
<li><p><strong>WaitEstimate</strong> Host’s wait time estimate</p></li>
</ol>
<p>There are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>6</mn></msup><mo>×</mo><msup><mn>3</mn><mn>2</mn></msup><mo>×</mo><msup><mn>4</mn><mn>2</mn></msup><mo>=</mo><mn>9</mn><mo>,</mo><mn>216</mn></mrow><annotation encoding="application/x-tex">2^6 \times 3^2 \times 4^2 = 9,216</annotation></semantics></math>
combinations for the input. Suppose we are given 12 inputs. We have to
make a best guess for the missing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><mo>,</mo><mn>204</mn></mrow><annotation encoding="application/x-tex">9,204</annotation></semantics></math>
cases using only these 12 examples.</p>
</body>
</html>
