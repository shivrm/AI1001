<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Shivram S ai24btech11031@iith.ac.in" />
  <title>AI1001 - Assignment 10</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">AI1001 - Assignment 10</h1>
<p class="author">Shivram S<br />
<code>ai24btech11031@iith.ac.in</code></p>
</header>
<h1 id="univariate-linear-regression">Univariate Linear Regression</h1>
<p>A univariate linear function with input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
has the form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">y = w_1x + w_0</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mn>0</mn></msub><annotation encoding="application/x-tex">w_0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mn>1</mn></msub><annotation encoding="application/x-tex">w_1</annotation></semantics></math>
are real-valued coefficients (<strong>weights</strong>) to be learned.
We define
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ğ°</mtext><annotation encoding="application/x-tex">\textbf{w}</annotation></semantics></math>
to be the vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false" form="postfix">âŸ©</mo></mrow><annotation encoding="application/x-tex">\langle w_0, w_1 \rangle</annotation></semantics></math>
and define the linear function as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_{\textbf{w}} = w_1x + w_0</annotation></semantics></math>.
The task of finding the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><annotation encoding="application/x-tex">h_{\textbf{w}}</annotation></semantics></math>
that best fits the data is called <strong>linear
regression</strong>.</p>
<p>To fit a line to the data, we find the values of the weights that
minimizes the empirical loss. We can calculate the empirical loss, using
the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math>
loss function, as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mi>i</mi></mrow><mi>N</mi></munderover><msub><mi>L</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo>âˆ’</mo><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">Loss(h_{\textbf{w}}) = \sum_{j=i}^N L_2(y_1, h_{\textbf{w}})(x_j) = \sum_{j=1}^N (y_j - h_{\textbf{w}}(x_j))^2</annotation></semantics></math></p>
<p>We obtain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">ğ°</mtext><mo>*</mo></msup><mo>=</mo><msub><mo>argmin</mo><mtext mathvariant="bold">ğ°</mtext></msub><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textbf{w}^* = \mathop{\mathrm{argmin}}_{\textbf{w}} Loss(h_{\textbf{w}})</annotation></semantics></math>
when the empirical loss is minimum and its partial derivatives with
respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mn>0</mn></msub><annotation encoding="application/x-tex">w_0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mn>1</mn></msub><annotation encoding="application/x-tex">w_1</annotation></semantics></math>
are zero.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>âˆ‚</mi><mrow><mi>âˆ‚</mi><msub><mi>w</mi><mn>0</mn></msub></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo>âˆ’</mo><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mn>0</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><mfrac><mi>âˆ‚</mi><mrow><mi>âˆ‚</mi><msub><mi>w</mi><mn>1</mn></msub></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo>âˆ’</mo><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\frac{\partial}{\partial w_0} \sum_{j=1}^N (y_j - h_{\textbf{w}}(x_j))^2 = 0
\text{ and } \frac{\partial}{\partial w_1} \sum_{j=1}^N (y_j - h_{\textbf{w}}(x_j))^2 = 0</annotation></semantics></math></p>
<p>These equations have a unique solution</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>=</mo><mfrac><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>âˆ‘</mo><msub><mi>x</mi><mi>j</mi></msub><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>âˆ‘</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>âˆ‘</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>âˆ‘</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>âˆ‘</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></mfrac><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mo>âˆ‘</mo><msub><mi>y</mi><mi>j</mi></msub><mo>âˆ’</mo><msub><mi>w</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mo>âˆ‘</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">w_1 = \frac{N(\sum x_jy_j) - (\sum x_j)(\sum y_j)}{N(\sum x_j^2) - (\sum x_j)^2}
\text{ and } w_0 = \frac{\left(\sum y_j - w_1\left(\sum x_j \right) \right)}{N}</annotation></semantics></math></p>
<p>The space defined by all possible settings of the weights is called
the <strong>weight space</strong>. For univariate linear regression the
weight space is two-dimensional. The loss function is
<strong>convex</strong> for every linear regression problem with the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math>
loss function. There are no local minima.</p>
<h1 id="gradient-descent">Gradient Descent</h1>
<p><strong>Gradient descent</strong> is a method that lets us minimize
loss without solving for the zeroes of the derivatives. It involves
computing an estimate of the gradient at a point, and moving downhill
along the steepest path until we reach a local minimum.</p>
<p>Gradient descent was discovered by Cauchy, who noticed that for any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo>,</mo><mi>â€¦</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u = f(x, y, z, \dots)</annotation></semantics></math>,
we could say that:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>Î±</mi><mo>,</mo><mi>y</mi><mo>+</mo><mi>Î²</mi><mo>,</mo><mi>z</mi><mo>+</mo><mi>Î³</mi><mo>,</mo><mi>â€¦</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>â‰ˆ</mo><mi>u</mi><mo>+</mo><mi>Î±</mi><mi>X</mi><mo>+</mo><mi>Î²</mi><mi>Y</mi><mo>+</mo><mi>Î³</mi><mi>Z</mi><mo>+</mo><mi>â€¦</mi></mrow><annotation encoding="application/x-tex">f(x + \alpha, y + \beta, z + \gamma, \dots) \approx u + \alpha X + \beta Y + \gamma Z + \dots</annotation></semantics></math></p>
<p>Where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><msub><mi>f</mi><mi>x</mi></msub><mi>â€²</mi></mrow><annotation encoding="application/x-tex">X = f_x&#39;</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><msub><mi>f</mi><mi>y</mi></msub><mi>â€²</mi></mrow><annotation encoding="application/x-tex">Y = f_y&#39;</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>â€¦</mi><annotation encoding="application/x-tex">\dots</annotation></semantics></math>
are the partial derivatives of the function. We can take
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î±</mi><mo>=</mo><mo>âˆ’</mo><mi>Î¸</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">\alpha = -\theta X</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î²</mi><mo>=</mo><mo>âˆ’</mo><mi>Î¸</mi><mi>Y</mi></mrow><annotation encoding="application/x-tex">\beta = -\theta Y</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>â€¦</mi><annotation encoding="application/x-tex">\dots</annotation></semantics></math>
to obtain</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>âˆ’</mo><mi>Î±</mi><mi>X</mi><mo>,</mo><mi>y</mi><mo>âˆ’</mo><mi>Î²</mi><mi>Y</mi><mo>,</mo><mi>z</mi><mo>âˆ’</mo><mi>Î²</mi><mi>Z</mi><mo>,</mo><mi>â€¦</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>â‰ˆ</mo><mi>u</mi><mo>âˆ’</mo><mi>Î¸</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>X</mi><mn>2</mn></msup><mo>+</mo><msup><mi>Y</mi><mn>2</mn></msup><mo>+</mo><msup><mi>Z</mi><mn>2</mn></msup><mo>+</mo><mi>â€¦</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x - \alpha X, y - \beta Y, z - \beta Z, \dots) \approx u - \theta(X^2 + Y^2 + Z^2 + \dots)</annotation></semantics></math></p>
<p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î¸</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is small enough, we get a value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î˜</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>âˆ’</mo><mi>Î¸</mi><mi>X</mi><mo>,</mo><mi>y</mi><mo>âˆ’</mo><mi>Î¸</mi><mi>Y</mi><mo>,</mo><mi>z</mi><mo>âˆ’</mo><mi>Î¸</mi><mi>Z</mi><mo>,</mo><mi>â€¦</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta = f(x - \theta X, y - \theta Y, z - \theta Z, \dots)</annotation></semantics></math>
which is less than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>.
We can repeat this until
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î˜</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math>
vanishes or coincides with a minimum value.</p>
<p>In the case of linear regression, we can iteratively reach
convergence by performing the operation
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>â†</mo><msub><mi>w</mi><mn>1</mn></msub><mo>âˆ’</mo><mi>Î±</mi><mfrac><mi>âˆ‚</mi><mrow><mi>âˆ‚</mi><msub><mi>w</mi><mn>1</mn></msub></mrow></mfrac><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ°</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">w_1 \gets w_1 - \alpha \frac{\partial}{\partial w_1} Loss(\textbf{w})</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
is a parameter called the <strong>learning rate</strong>. It can be a
fixed constant or can decay over time. We can substitute the values of
the partial derivatives to get</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>â†</mo><msub><mi>w</mi><mi>o</mi></msub><mo>+</mo><mi>Î±</mi><munder><mo>âˆ‘</mo><mi>j</mi></munder><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>âˆ’</mo><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>w</mi><mn>1</mn></msub><mo>â†</mo><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><mi>Î±</mi><munder><mo>âˆ‘</mo><mi>j</mi></munder><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>âˆ’</mo><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>Ã—</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_0 \gets w_o + \alpha \sum_j (y - h_{\textbf{w}}(x_j))
\text{ and } w_1 \gets w_1 + \alpha \sum_j(y - h_{\textbf{w}}(x_j)) \times x_j</annotation></semantics></math></p>
<p>These updates are called the <strong>batch gradient descent</strong>
learning rule for univariate linear regression (also called
<strong>deterministic gradient descent</strong>). A step that covers all
the training examples is called an <strong>epoch</strong>.</p>
<p>A faster variant, called <strong>stochastic gradient descent</strong>
(SGD), randomly selects a small number of training examples at each
step. For example, by taking a <strong>minibatch</strong> size of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>/</mi><mn>100</mn></mrow><annotation encoding="application/x-tex">N/100</annotation></semantics></math>,
each step becomes 100 times faster. Since the error is proportional to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mi>N</mi></msqrt><annotation encoding="application/x-tex">\sqrt{N}</annotation></semantics></math>,
we need 10 times as many steps. Overall, this method is still 10 times
faster than deterministic gradient descent.</p>
<p>Convergence of SGD is not necessarily guaranteed. It can oscillate
around the minimum without converging. However, gradual decrement of the
learning rate,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>,
can guarantee convergence. SGD is also useful in an online setting,
where new data comes in one at a time, and a model needs to adapt to
changes represented in the new data.</p>
<h1 id="multivariate-linear-regression">Multivariate Linear
Regression</h1>
<p>We can extend our approach to <strong>multivariate linear
regression</strong> where each example
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="bold">ğ±</mtext><mi>j</mi></msub><annotation encoding="application/x-tex">\textbf{x}_j</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-element
vector, and out hypotheses are functions of the form
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">ğ±</mtext><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mrow><mi>j</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>â€¦</mi><mo>+</mo><msub><mi>w</mi><mi>n</mi></msub><msub><mi>x</mi><mrow><mi>j</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>=</mo><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><munder><mo>âˆ‘</mo><mi>i</mi></munder><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_{\textbf{w}}(\textbf{x}_j) = w_0 + w_1x_{j,1} + \dots + w_nx_{j,n} = w_0 + \sum_i w_i x_{j,i}</annotation></semantics></math></p>
<p>If we invent a dummy attribute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>j</mi><mo>,</mo><mn>0</mn></mrow></msub><annotation encoding="application/x-tex">x_{j,0}</annotation></semantics></math>,
which is always equal to 1, we can write our hypotheses as dot products
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mtext mathvariant="bold">ğ°</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">ğ±</mtext><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="bold">ğ°</mtext><mo>â‹…</mo><msub><mtext mathvariant="bold">ğ±</mtext><mi>j</mi></msub><mo>=</mo><msup><mtext mathvariant="bold">ğ°</mtext><mi>âŠ¤</mi></msup><msub><mtext mathvariant="bold">ğ±</mtext><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_{\textbf{w}}(\textbf{x}_j) = \textbf{w} \cdot \textbf{x}_j = \textbf{w}^\top \textbf{x}_j</annotation></semantics></math></p>
<p>The best vector of weights,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mtext mathvariant="bold">ğ°</mtext><mo>*</mo></msup><annotation encoding="application/x-tex">\textbf{w}^*</annotation></semantics></math>
minimizes squared-error:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">ğ°</mtext><mo>*</mo></msup><mo>=</mo><msub><mo>argmin</mo><mtext mathvariant="bold">ğ°</mtext></msub><munder><mo>âˆ‘</mo><mi>j</mi></munder><msub><mi>L</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo>,</mo><mtext mathvariant="bold">ğ°</mtext><mo>â‹…</mo><msub><mtext mathvariant="bold">ğ±</mtext><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textbf{w}^* = \mathop{\mathrm{argmin}}_{\textbf{w}} \sum_j L_2(y_j, \textbf{w} \cdot \textbf{x}_j)</annotation></semantics></math></p>
<p>It is possible to solve analytically for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ğ°</mtext><annotation encoding="application/x-tex">\textbf{w}</annotation></semantics></math>
that minimizes loss. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ğ²</mtext><annotation encoding="application/x-tex">\textbf{y}</annotation></semantics></math>
is the vector of outputs and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">ğ—</mtext><annotation encoding="application/x-tex">\textbf{X}</annotation></semantics></math>
be the <strong>data matrix</strong>, then the vector of predicted
outputs is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>ğ²</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><mtext mathvariant="bold">ğ—ğ°</mtext></mrow><annotation encoding="application/x-tex">\mathbf{\hat y} = \textbf{Xw}</annotation></semantics></math>,
and the squared-error loss over the training data is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ°</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="prefix">âˆ¥</mo><mover><mi>ğ²</mi><mo accent="true">Ì‚</mo></mover><mo>âˆ’</mo><mtext mathvariant="bold">ğ²</mtext><msup><mo stretchy="false" form="postfix">âˆ¥</mo><mn>2</mn></msup><mo>=</mo><mo stretchy="false" form="prefix">âˆ¥</mo><mtext mathvariant="bold">ğ—ğ°</mtext><mo>âˆ’</mo><mtext mathvariant="bold">ğ²</mtext><msup><mo stretchy="false" form="postfix">âˆ¥</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(\textbf{w}) = \lVert \mathbf{\hat y} - \textbf{y}\rVert^2 = \lVert \textbf{Xw} - \textbf{y}\rVert^2</annotation></semantics></math></p>
<p>We can set the gradient to zero</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>âˆ‡</mi><mtext mathvariant="bold">ğ°</mtext></msub><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ°</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><msup><mtext mathvariant="bold">ğ—</mtext><mi>âŠ¤</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ—ğ°</mtext><mo>âˆ’</mo><mtext mathvariant="bold">ğ²</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_{\textbf{w}} L(\textbf{w}) = 2 \textbf{X}^\top (\textbf{Xw} - \textbf{y}) = 0</annotation></semantics></math></p>
<p>Rearranging, we find that the minimum-loss weight vector is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">ğ°</mtext><mo>*</mo></msup><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ğ—</mtext><mi>âŠ¤</mi></msup><mtext mathvariant="bold">ğ—</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><msup><mtext mathvariant="bold">ğ—</mtext><mi>âŠ¤</mi></msup><mtext mathvariant="bold">ğ²</mtext></mrow><annotation encoding="application/x-tex">\textbf{w}^* = (\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \textbf{y}</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ğ—</mtext><mi>âŠ¤</mi></msup><mtext mathvariant="bold">ğ—</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><msup><mtext mathvariant="bold">ğ—</mtext><mi>âŠ¤</mi></msup></mrow><annotation encoding="application/x-tex">(\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top</annotation></semantics></math>
is called the <strong>pseudoinverse</strong> of the matrix, and the
above equation is called the <strong>normal equation</strong>.</p>
</body>
</html>
