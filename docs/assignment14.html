<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Shivram S ai24btech11031@iith.ac.in" />
  <title>AI1001 - Assignment 14</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">AI1001 - Assignment 14</h1>
<p class="author">Shivram S<br />
<code>ai24btech11031@iith.ac.in</code></p>
</header>
<h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>
<p><strong>Recurrent neural networks</strong> (RNNs) allow cycles in
computation graphs. Units may take as input values computed from their
own output at an earlier step. This allows the RNN to have internal
state or <strong>memory</strong>.</p>
<p>RNNs can be used as tools for analyzing sequential data, similar to
hidden Markov models, dynamic Bayesian networks, and Kalman filters.
RNNs make a <strong>Markov assumption</strong>: that the hidden state
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="bold">𝐳</mtext><mi>t</mi></msub><annotation encoding="application/x-tex">\textbf{z}_t</annotation></semantics></math>
of the network captures information from all previous inputs.
Additionally, the function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mtext mathvariant="bold">𝐰</mtext></msub><annotation encoding="application/x-tex">f_{\textbf{w}}</annotation></semantics></math>
in the RNN’s update process
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">𝐳</mtext><mi>t</mi></msub><mo>=</mo><msub><mi>f</mi><mtext mathvariant="bold">𝐰</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐳</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mtext mathvariant="bold">𝐱</mtext><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textbf{z}_t = f_{\textbf{w}}(\textbf{z}_{t-1}, \textbf{x}_t)</annotation></semantics></math>
must represent a <strong>time-homogeneous</strong> process.</p>
<p>If we used a feedforward network to analyze sequential data, the
network could examine only a finite-length window of the data, and the
network would fail to detect long-distance dependencies. RNNs address
this by keeping track of previous inputs in the hidden state.</p>
<h2 id="training-an-rnn">Training an RNN</h2>
<p>We will consider a model with an input later
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
a hidden layer
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
with recurrent connections, and an output layer
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐲</mtext><annotation encoding="application/x-tex">\textbf{y}</annotation></semantics></math>.
We can define the model using the equations:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="bold">𝐳</mtext><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>f</mi><mtext mathvariant="bold">𝐰</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐳</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mtext mathvariant="bold">𝐳</mtext><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mtext mathvariant="bold">𝐠</mtext><mi>z</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub><msub><mtext mathvariant="bold">𝐳</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub><msub><mtext mathvariant="bold">𝐱</mtext><mi>y</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≡</mo><msub><mtext mathvariant="bold">𝐠</mtext><mi>z</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐢𝐧</mtext><mrow><mi>z</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mover><mi>𝐲</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mtext mathvariant="bold">𝐠</mtext><mi>y</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>z</mi><mo>,</mo><mi>y</mi></mrow></msub><msub><mtext mathvariant="bold">𝐳</mtext><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≡</mo><msub><mtext mathvariant="bold">𝐠</mtext><mi>y</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐢𝐧</mtext><mrow><mi>y</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\textbf{z}_t  &amp;= f_{\textbf{w}}(\textbf{z}_{t-1}, \textbf{z}_t) = \textbf{g}_z(\textbf{W}_{z,z}\textbf{z}_{t-1} + \textbf{W}_{x,z}\textbf{x}_y) \equiv \textbf{g}_z(\textbf{in}_{z,t})\\
\mathbf{\hat y}_t &amp;= \textbf{g}_y(\textbf{W}_{z,y}\textbf{z}_t) \equiv \textbf{g}_y(\textbf{in}_{y,t})
\end{aligned}</annotation></semantics></math></p>
<p>Given a sequence of input vectors
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">𝐱</mtext><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mtext mathvariant="bold">𝐱</mtext><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{x}_1, \dots, \textbf{x}_T</annotation></semantics></math>
and the observed outputs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">𝐲</mtext><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mtext mathvariant="bold">𝐲</mtext><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{y}_1, \dots, \textbf{y}_T</annotation></semantics></math>,
We can “unroll” the RNN into a feedforward network. The weight matrices
are shared across all time steps. We can calculate gradients to train
the weights using gradient descent, but the sharing of weights makes the
computation a little more complicated. For example, to calculate the
gradient for the hidden layer weight
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub><annotation encoding="application/x-tex">w_{z,z}</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mo>−</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mfrac><mrow><mi>∂</mi><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mo>−</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mfrac><mrow><mi>∂</mi><mrow></mrow></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub></mrow></mfrac><msub><mi>g</mi><mi>y</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><msub><mi>n</mi><mrow><mi>y</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mo>−</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>g</mi><mi>y</mi></msub><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><msub><mi>n</mi><mrow><mi>y</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>y</mi></mrow></msub><mfrac><mrow><mi>∂</mi><msub><mi>z</mi><mi>t</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \frac{\partial{L}}{\partial{w_{z,z}}} &amp;= \sum_{t=1}^T -2(y_t - \hat y_t) \frac{\partial{\hat y_t}}{\partial{w_{z,z}}} \\
    &amp;= \sum_{t=1}^T -2(y_t - \hat y_t) \frac{\partial{}}{\partial{w_{z,z}}} g_y(in_{y,t}) \\
    &amp;= \sum_{t=1}^T -2(y_t - \hat y_t)g_y&#39;(in_{y,t}) w_{z,y} \frac{\partial{z_t}}{\partial{w_{z,z}}}
\end{aligned}</annotation></semantics></math></p>
<p>We can now calculate the gradient for the hidden unit
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_t</annotation></semantics></math>
as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><msub><mi>z</mi><mi>t</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mrow></mrow></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub></mrow></mfrac><msub><mi>g</mi><mi>z</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><msub><mi>n</mi><mrow><mi>z</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>g</mi><mi>z</mi></msub><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><msub><mi>n</mi><mrow><mi>z</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub><mfrac><mrow><mi>∂</mi><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>∂</mi><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial{z_t}}{\partial{w_{z,z}}} = \frac{\partial{}}{\partial{w_{z,z}}} g_z(in_{z,t}) = g_z&#39;(in_{z,t}) \left(z_{t-1} + w_{z,z} \frac{\partial{z_{t-1}}}{\partial{w_{z,z}}} \right)</annotation></semantics></math></p>
<p>Since the contribution to the gradient from time step
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>
is calculated using the contribution from time step
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t - 1</annotation></semantics></math>,
the algorithm runs in linear time and is called <strong>back-propagation
through time</strong>.</p>
<p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>z</mi><mo>,</mo><mi>z</mi></mrow></msub><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w_{z,z} &gt; 1</annotation></semantics></math>,
we may experience the <strong>exploding gradient problem</strong>. This
can be mitigated using more elaborate RNN design.</p>
<h2 id="long-short-term-memory-rnns">Long short-term memory RNNs</h2>
<p><strong>Long short-term memory</strong> (LSTM) RNNs are architectures
designed to preserve information over many time steps. The long term
memory has a <strong>memory cell</strong>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐜</mtext><annotation encoding="application/x-tex">\textbf{c}</annotation></semantics></math>,
which is copied from time step to time step. New information enters the
memory by adding updates. LSTMs also include <strong>gating
units</strong> that control the flow of information.</p>
<ul>
<li><p>The <strong>forget gate</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐟</mtext><annotation encoding="application/x-tex">\textbf{f}</annotation></semantics></math>
determines if each element of the memory cell is remembered or
forgotten.</p></li>
<li><p>The <strong>input gate</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐢</mtext><annotation encoding="application/x-tex">\textbf{i}</annotation></semantics></math>
determines if each element of the memory cell is updated additively by
new information from the input vector.</p></li>
<li><p>The <strong>output gate</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐨</mtext><annotation encoding="application/x-tex">\textbf{o}</annotation></semantics></math>
determines if each element of the memory cell is transferred to the
short-term memory
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>.</p></li>
</ul>
<p>Unlike Boolean functions, gates in LSTM are soft. For example,
elements of the memory cell will be partially forgotten if the elements
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐟</mtext><annotation encoding="application/x-tex">\textbf{f}</annotation></semantics></math>
are small, but not zero. Values of gating units are always in the range
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math>
as they are obtained as outputs of a sigmoid function. We can write the
update equations for the gating units as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="bold">𝐟</mtext><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>x</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mtext mathvariant="bold">𝐱</mtext><mi>t</mi></msub><mo>+</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>z</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mtext mathvariant="bold">𝐳</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="bold">𝐢</mtext><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>x</mi><mo>,</mo><mi>i</mi></mrow></msub><msub><mtext mathvariant="bold">𝐱</mtext><mi>t</mi></msub><mo>+</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>z</mi><mo>,</mo><mi>i</mi></mrow></msub><msub><mtext mathvariant="bold">𝐳</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="bold">𝐨</mtext><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>x</mi><mo>,</mo><mi>o</mi></mrow></msub><msub><mtext mathvariant="bold">𝐱</mtext><mi>t</mi></msub><mo>+</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>z</mi><mo>,</mo><mi>o</mi></mrow></msub><msub><mtext mathvariant="bold">𝐳</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\textbf{f}_t &amp;= \sigma(\textbf{W}_{x,f}\textbf{x}_t + \textbf{W}_{z,f}\textbf{z}_{t-1}) \\
\textbf{i}_t &amp;= \sigma(\textbf{W}_{x,i}\textbf{x}_t + \textbf{W}_{z,i}\textbf{z}_{t-1}) \\
\textbf{o}_t &amp;= \sigma(\textbf{W}_{x,o}\textbf{x}_t + \textbf{W}_{z,o}\textbf{z}_{t-1})
\end{aligned}</annotation></semantics></math></p>
<p>The update rules for the memory are given by</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="bold">𝐜</mtext><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mtext mathvariant="bold">𝐟</mtext><mi>t</mi></msub><mo>⊙</mo><msub><mtext mathvariant="bold">𝐜</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mtext mathvariant="bold">𝐢</mtext><mi>t</mi></msub><mo>⊙</mo><mo>tanh</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>x</mi><mo>,</mo><mi>c</mi></mrow></msub><msub><mtext mathvariant="bold">𝐱</mtext><mi>t</mi></msub><mo>+</mo><msub><mtext mathvariant="bold">𝐖</mtext><mrow><mi>z</mi><mo>,</mo><mi>c</mi></mrow></msub><msub><mtext mathvariant="bold">𝐳</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="bold">𝐳</mtext><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mtext mathvariant="bold">𝐨</mtext><mi>t</mi></msub><mo>⊙</mo><mo>tanh</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="bold">𝐜</mtext><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\textbf{c}_t &amp;= \textbf{f}_t \odot \textbf{c}_{t-1} + \textbf{i}_t \odot \tanh(\textbf{W}_{x,c} \textbf{x}_t + \textbf{W}_{z,c} \textbf{z}_{t-1})  \\
\textbf{z}_t &amp;= \textbf{o}_t \odot \tanh(\textbf{c}_t)
\end{aligned}</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math>
denotes elementwise multiplication.</p>
<h1 id="unsupervised-learning">Unsupervised Learning</h1>
<p>Supervised learning algorithms are given a training set of inputs and
corresponding outputs. Unsupervised learning algorithms, on the other
hand, take a training set of unlabeled examples
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>.
The algorithm might try to learn new representations, or it might learn
a generative model from which new samples can be generated.</p>
<p>Suppose we learn a joint model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>W</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo>,</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_W(\textbf{x}, \textbf{z})</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
is a set of latent, unobserved variables that represent the content of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
in some way. The model can associate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
however it chooses. The model can achieve both <strong>representation
learning</strong> (constructing meaningful
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>)
and <strong>generative modelling</strong> (determining
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>W</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_W(\textbf{x})</annotation></semantics></math>)</p>
<h2 id="probabilistic-pca">Probabilistic PCA</h2>
<p>In a <strong>probabilistic principal component analysis</strong>
(PPCA),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
is chosen from a zero-mean, spherical Gaussian.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
is generated from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
by applying a weight matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐖</mtext><annotation encoding="application/x-tex">\textbf{W}</annotation></semantics></math>
and adding spherical Gaussian noise (with noise parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>).
The weights can be learnt by maximizing the likelihood of the data.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>W</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>∫</mo><msub><mi>P</mi><mi>W</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo>,</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝒩</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo>;</mo><mtext mathvariant="bold">𝟎</mtext><mo>,</mo><mtext mathvariant="bold">𝐖</mtext><msup><mtext mathvariant="bold">𝐖</mtext><mi>⊤</mi></msup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup><mtext mathvariant="bold">𝐈</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_W(\textbf{x}) = \int P_W(\textbf{x}, \textbf{z}) = \mathcal{N}(\textbf{x}; \textbf{0}, \textbf{W}\textbf{W}^\top + \sigma^2\textbf{I})</annotation></semantics></math></p>
<p>This can be done by gradient methods, or by an efficient EM
algorithm. Once
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐖</mtext><annotation encoding="application/x-tex">\textbf{W}</annotation></semantics></math>
has been learned, new data samples can be generated directly from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>W</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_W(\textbf{x})</annotation></semantics></math>.
Additionally, new observations that have very low probability can be
flagged as potential anomalies.</p>
<p>The dimensionality of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
is much less than the dimensionality of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>,
so the model learns to explain the data in terms of a small number of
features. These features can be extracted for use in classifiers by
computing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝐳</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\mathbf{\hat z}</annotation></semantics></math>,
the expectation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>W</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_W(\textbf{z} | \textbf{x})</annotation></semantics></math>.</p>
<p>We can generate data from a PPCA model by sampling
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
from the Gaussian prior, then sampling
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
from a Gaussian with mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐖𝐳</mtext><annotation encoding="application/x-tex">\textbf{Wz}</annotation></semantics></math>.</p>
<h2 id="autoencoders">Autoencoders</h2>
<p>An <strong>autoencoder</strong> is a model containing two parts: an
encoder that maps from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
to a representation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝐳</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\mathbf{\hat z}</annotation></semantics></math>,
and a decoder that maps from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝐳</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\mathbf{\hat z}</annotation></semantics></math>
to observed data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>.
The model is trained so that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">𝐱</mtext><mo>≈</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textbf{x} \approx g(f(\textbf{x}))</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
is the encoder function, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
is the decoder function.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
may be simple linear models, or they can be represented by a deep neural
network.</p>
<p>The linear autoencoder is a simple autoencoder where:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mover><mi>𝐳</mi><mo accent="true">̂</mo></mover></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="bold">𝐖</mtext><mtext mathvariant="bold">𝐱</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="bold">𝐱</mtext></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>𝐳</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mtext mathvariant="bold">𝐖</mtext><mi>⊤</mi></msup><mover><mi>𝐳</mi><mo accent="true">̂</mo></mover></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \mathbf{\hat z} &amp;= f(\textbf{x}) = \textbf{W} \textbf{x} \\
    \textbf{x} &amp;= g(\mathbf{\hat z}) = \textbf{W}^\top \mathbf{\hat z}
\end{aligned}</annotation></semantics></math></p>
<p>This model can be trained by minimizing the squared error. The idea
is to train
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐖</mtext><annotation encoding="application/x-tex">\textbf{W}</annotation></semantics></math>
so that a low-dimensional
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝐳</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\mathbf{\hat z}</annotation></semantics></math>
will retain as much information as possible to reconstruct
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>.</p>
<p>When
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>-dimensional,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐖</mtext><annotation encoding="application/x-tex">\textbf{W}</annotation></semantics></math>
must learn to span the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
principal components of the data, which are the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
eigenvectors that have the largest eigenvalues. Thus, the linear
autoencoder turns out to be closely connected to classical PCA.</p>
<p>The correspondence between classical PCA models and linear
autoencoders suggest that there might be a way to capture more complex
kinds of generative models using more complex autoencoders. The
<strong>variational autoencoder</strong> (VAE) provides one way to do
it.The idea is to use a <strong>variational posterior</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q(\textbf{z})</annotation></semantics></math>
drawn from a computationally tractable family of distributions as an
approximation to the true posterior.</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
is optimized to be “as close as possible” to the true distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\textbf{z} | \textbf{x})</annotation></semantics></math>.
This is done by minimizing KL divergence:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="postfix">‖</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>∫</mo><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><mfrac><mrow><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>d</mi><mtext mathvariant="bold">𝐳</mtext></mrow><annotation encoding="application/x-tex">D_{KL}(Q(\textbf{z}) \Vert P(\textbf{z} | \textbf{x})) = \int Q(\textbf{z}) \log \frac{Q(\textbf{z})}{P(\textbf{z} | \textbf{x})} d\textbf{z}</annotation></semantics></math></p>
<p>KL divergence is zero when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
conincide, and positive otherwise. We can define the <strong>variational
lower bound</strong> or <strong>evidence lower bound</strong> (ELBO),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal L</annotation></semantics></math>
as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo>,</mo><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="postfix">‖</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal L(\textbf{x}, Q) = \log P(\textbf{x}) - D_{KL}(Q(\textbf{x}) \Vert P(\textbf{z} | \textbf{x}))</annotation></semantics></math></p>
<p>Variational learning maximizes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal L</annotation></semantics></math>
in the hope that the solution will be close to maximizing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log P(\textbf{x})</annotation></semantics></math>.
We may rewrite the expression for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal L</annotation></semantics></math>
as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℒ</mi><mo>=</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mtext mathvariant="bold">𝐄</mtext><mrow><mtext mathvariant="bold">𝐳</mtext><mo>∼</mo><mi>Q</mi></mrow></msub><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo>,</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal L = H(Q) + \textbf{E}_{\textbf{z} \sim Q} \log P(\textbf{z}, \textbf{x})</annotation></semantics></math></p>
<p>For some
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
(such as Gaussian distributions),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(Q)</annotation></semantics></math>
can be evaluated analytically.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">𝐄</mtext><mrow><mtext mathvariant="bold">𝐳</mtext><mo>∼</mo><mi>Q</mi></mrow></msub><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo>,</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textbf{E}_{\textbf{z} \sim Q} \log P(\textbf{z}, \textbf{x})</annotation></semantics></math>
can be estimated using samples of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo>,</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\textbf{z}, \textbf{x})</annotation></semantics></math>
can usually be evaluated efficiently: for example if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
is a Bayes net, then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐳</mtext><mo>,</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\textbf{z}, \textbf{x})</annotation></semantics></math>
is just a product of conditional probabilities.</p>
<h2 id="deep-autoregressive-models">Deep autoregressive models</h2>
<p>An <strong>autoregressive model</strong> is one in which each element
of the data vector is predicted based on other elements of the vector.
If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
is of fixed size, an AR model can be thought of as a fully observable
and possibly fully connected Bayes net. This makes it easy to calculate
the likelihood of a given data vector, and to predict the value of a
simple missing variable, given all others.</p>
<p>AR models are commonly used in the analysis of time series data,
where an AR model of order
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
predicts
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_t</annotation></semantics></math>
given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mi>k</mi></mrow></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{t-k}, \dots, x_{t-1}</annotation></semantics></math>.
Hence, an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-gram
model is an AR model of order
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math>.</p>
<p>In classical AR models, the conditional distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mi>k</mi></mrow></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(x_t | x_{t-k}, \dots, x_{t-1})</annotation></semantics></math>
is a linear Gaussian model with a fixed variance whose mean is a
weighted linear combination of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mi>k</mi></mrow></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{t-k}, \dots, x_{t-1}</annotation></semantics></math>,
i.e., a standard linear regression model. The maximum likelihood is
given by the <strong>Yule-Walker</strong> equations.</p>
<p>A <strong>deep autoregressive model</strong> is one in which the
linear Gaussian model is replaced by an arbitrary deep network with a
suitable output layer. Recent applications of deep autoregressive models
include DeepMind’S WaveNet speech generation model, which implements a
nonlinear AR model of order 4800 with a convolutional structure.</p>
<h2 id="generative-adversarial-networks">Generative adversarial
networks</h2>
<p>A <strong>generative adversarial network</strong> (GAN) is a pair of
networks: a <strong>generator</strong> which maps values from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐳</mtext><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
to produce samples from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>W</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_W(\textbf{x})</annotation></semantics></math>,
and a <strong>discriminator</strong>, which is a classifier trained to
classify
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
as real (from the training set), or fake (created by the generator).
GANs are <strong>implicit models</strong>, in that samples can be
generated, but probabilities are not available.</p>
<p>The generator of a GAN is closely related to the decoder of a VAE.
Both the generator and the discriminator are trained simultaneously. The
competition between generator and discriminator can be described using
game theory. The idea is that in the equilibrium state, the generator
should reproduce the training distribution perfectly, so that the
discriminator can’t perform better than random guessing.</p>
<p>GANs have worked particularly well for image-generation tasks.</p>
<h2 id="unsupervised-translation">Unsupervised translation</h2>
<p>Translation tasks involves multidimensional data such as images and
natural language which have statistical dependence between the various
dimensions. Such data is said to have “rich structure.” Translation
tasks consist of transforming an input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
that has rich structure to an output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐲</mtext><annotation encoding="application/x-tex">\textbf{y}</annotation></semantics></math>
that also has rich structure.</p>
<p>In supervised translation, the data consists of many
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo>,</mo><mtext mathvariant="bold">𝐲</mtext><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\textbf{x}, \textbf{y})</annotation></semantics></math>
pairs, and the model maps each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
to the corresponding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐲</mtext><annotation encoding="application/x-tex">\textbf{y}</annotation></semantics></math>.
Unsupervised translation trains on many
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>
and separate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐲</mtext><annotation encoding="application/x-tex">\textbf{y}</annotation></semantics></math>,
but no corresponding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">𝐱</mtext><mo>,</mo><mtext mathvariant="bold">𝐲</mtext><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\textbf{x}, \textbf{y})</annotation></semantics></math>
pairs.</p>
<p>Most unsupervised translation approaches are based on GANs. The GAN
training framework makes it possible to train a generator that generates
any of the possible samples that the the discriminator accepts as a
realistic example of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐲</mtext><annotation encoding="application/x-tex">\textbf{y}</annotation></semantics></math>
given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐱</mtext><annotation encoding="application/x-tex">\textbf{x}</annotation></semantics></math>,
without any need for a specific paired
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="bold">𝐲</mtext><annotation encoding="application/x-tex">\textbf{y}</annotation></semantics></math>.</p>
<h2 id="transfer-learning-and-multitask-learning">Transfer learning and
multitask learning</h2>
<p>In <strong>transfer learning</strong>, experience with one learned
task helps an agent learn better on another task. We may take a network
trained for a task A, and update its weights using gradient descent for
task B. We may use a smaller learning rate in task B, depending on the
similarity of the tasks, and how much data was used in task A.</p>
<p>There are many high-quality pre-trained models available, which has
contributed to the popularity of transfer learning. For example, using a
pre-trained visual object TODO: AR models recognition model such as
ResNet-50 can help with identification of relevant features, thus saving
weeks of training time.</p>
<p>If the tasks are very similar, we might also freeze the first few
layers of the model as they serve as feature detectors that will be
useful for the new model. The later layers are the ones that identify
problem-specific features.</p>
<p>In natural language processing, models such as RoBERTa are
pre-trained on the vocabulary and syntax of everyday language. Such
models can be fine-tuned with domain-specific vocabulary, and trained on
task-specific data. For example, a model for answering questions might
be trained on question/answer pairs.</p>
<p>The controller for a self-driving car might be trained on a
simulator, but in the real world, the environment can vary greatly. The
model can be fine-tuned with real-time data from an actual vehicle, and
thus it can adapt quickly to the new environment.</p>
<p><strong>Multitask learning</strong> is a form of transfer learning in
which we simultaneously train a model on multiple objectives. This
allows the model to create a “common representation” that reflects
similarities between the tasks. For example, a natural language model
might be trained simultaneously on part-of-speech tagging, document
classification, language detection, word prediction, etc.</p>
<h1 id="applications">Applications</h1>
<h2 id="vision">Vision</h2>
<p>Computer vision is the application area which had the biggest impact
on deep learning. Deep convolutional networks have been used in
handwriting recognition, speech generation, etc.</p>
<p>Deep learning was popularized by the AlexNet image classification
system, which managed to achieve an error rate of 15.3% on the ImageNet
competition. The model had five convolutional layers, interspersed with
max-pooling layers, followed by three fully connected layers. It took
advantage of GPUs to speed up the training process.</p>
<p>Since 2012, the top-5 error rate on ImageNet has been reduced to less
than 2%. CNNs have been applied in a wide range on visual tasks. Self
driving is among the most demanding of visual tasks.</p>
<h2 id="natural-language-processing">Natural language processing</h2>
<p>Deep learning has had a huge impact on tasks such as translation and
speech recognition. There is the possibility of end-to-end learning and
automatic generation of representations for the meanings of words.</p>
<p>In translation tasks, the classical pipeline approach, which
corresponds to how a human translator works, is outperfoemd by
end-to-end methods. Machine translation systems are approaching human
performance for languages pairs such as French and English, which have
large paired data sets available.</p>
<p>There is some evidence that networks trained on multiple languages
lean an internal meaning representations. For example, learning
Portuguese/English and English/Spanish translations have allowed models
to perform Portuguese/Spanish translations without any
Portuguese/Spanish training pairs.</p>
<p>The representation of words as vectors in a high-dimensional space,
known as <strong>word embeddings</strong>, has shown promise. Because
words with similar meanings are used in similar contexts, they end up
near each other in the vector space. This allows the network to
generalize across categories of words.</p>
<h2 id="reinforcement-learning">Reinforcement learning</h2>
<p>In reinforcement learning (RL), an agent learns from a series of
reward signals that provide some indication to the quality of its
behavior. The goal is to optimize the sum of future rewards. The agent
can learn a value function, a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>-function,
a policy, and so on.</p>
<p>While the methods of training in RL differ from those of supervised
learning, the ability of multilayer computation graphs to represent
complex functions over large input spaces has lead to the development of
the field of <strong>deep reinforcement learning</strong>.</p>
<p>The first major demonstration of deep RL was DeepMind’s Atari playing
agent, DQN. The agent learnt a Q-function from raw image data, with the
reward signal begin the game score. DeepMind’s AlphaGo system also used
deep RL to defeat the best human players at the game of Go.</p>
<p>Despite its successes, deep RL still faces significant obstacles. It
is often difficult to get good performance, and trained systems may
behave very unpredictably if the environment differs even a little from
training data.</p>
</body>
</html>
