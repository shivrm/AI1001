<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Shivram S ai24btech11031@iith.ac.in" />
  <title>AI1001 - Assignment 15</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">AI1001 - Assignment 15</h1>
<p class="author">Shivram S<br />
<code>ai24btech11031@iith.ac.in</code></p>
</header>
<h1 id="reinforcement-learning">Reinforcement Learning</h1>
<p>In <strong>reinforcement learning</strong>, the learner is not told
what actions to take, and must discover which actions yield the most
reward. The problem is formalized using concepts from dynamical systems
and optimal control of Markov decision processes.</p>
<p>In reinforcement learning there is a tradeoff between exploration and
exploitation. The agent can take actions that it has tried in the past
and found to be effective, but this prevents it from trying new actions
and discovering potentially better selections. The agent must try a
variety of actions and progressively favour those that appear to be
best.</p>
<p>Unlike other fields of ML which focus on isolated subproblems,
reinforcement learning focuses on the interactions of a decision-making
agent which seeks to achieve a goal despite uncertainty about its
environment. RL operates on simple general principles, and is close to
the kind of learning that humans and other animals do.</p>
<p>There are four major subelements of a reinforcement learning
system:</p>
<ul>
<li><p>The <strong>policy</strong> defines the agent’s behaviour at a
given time. Given the state of the element, it determines the action to
be taken. It may be a simple lookup table or an extensive search
process.</p></li>
<li><p>The <strong>reward signal</strong> is a number that determines if
the agent’s course of action was “good” or “bad”. It is the primary
basis for altering the agent’s policy.</p></li>
<li><p>The <strong>value function</strong> is a way to specify the
long-term value of a state. It is the total amount of reward the agent
can expect to accumulate over the future, starting from that state. It
is much harder to determine values, and value estimation is the most
important component of almost all RL algorithms.</p></li>
<li><p>An RL system might include a <strong>model</strong> of the
environment, which mimics how the environment might behave. Models are
used for <strong>planning</strong> - deciding an action by considering
possible future situations. There are also <strong>model-free</strong>
agents that only learn by trial-and-error.</p></li>
</ul>
<p>In an RL system, state is a crucial part of the policy and value
functions. Most RL systems are structured around estimating the value
functions, but other methods such as solution methods, genetic
algorithms, simulated annealing, etc. can also be used.</p>
<p><strong>Evolutionary methods</strong> apply multiple randomized
policies and use the policy which obtains the most reward as the
starting point for the next generation. However, these methods ignore
much of the useful structure of the reinforcement learning problem: they
don’t use the the fact that the policy is a function from states to
actions, and they don’t notice which states a system passes through
during its lifetime, or which actions it takes.</p>
<h1 id="tic-tac-toe">Tic-Tac-Toe</h1>
<p>Consider the game of tic-tac-toe against an imperfect opponent. We
want to construct a player that will find the imperfections. The player
must learn from experience rather than rely solely on predefined
strategies such as minimax. One approach involves learning a model of
the opponent’s behavior through repeated play, then using dynamic
programming to compute optimal moves based on the model.</p>
<p>Evolutionary methods can explore various policies through simulated
games and choose one with a high probability of winning. A typical
evolutionary method would <strong>hill-climb</strong> through
policy-space. Alternatively, a genetic algorithm could be used, which
would maintain and evaluate a population of policies.</p>
<p>For a method making use of a value function, we might set up a table
of numbers, one for each possible state of the game. Each number would
be the latest estimate of the state’s value. The initial values could be
assigned as 1 for states where we have won, 0 for states where we have
lost, and 0.5 for every other state.</p>
<p>We can play many games against the opponent, mostly moving
<strong>greedily</strong> by selecting the move that leads to the state
with greatest value. Occasionally, we can play
<strong>exploratively</strong> by selecting randomly from the other
moves.</p>
<p>At the end of the game, we can "back up" through our tree of moves,
and update the value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>
of the state
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding="application/x-tex">S_t</annotation></semantics></math>
to bring it closer to the value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>
of the next state
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>←</mo><mi>V</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>α</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>V</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>V</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">V(S_t) \gets V(S_t) + \alpha \left[ V(S_{t+1}) - V(S_t)\right]</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
is a very small positive fraction called the <strong>step-size</strong>
parameter, which influences the rate of learning. This update rule is an
example of a <strong>temporal-difference</strong> learning method.</p>
<p>If the step-size parameter is reduced over time, then the model
converges to any fixed opponent. If the parameter is not reduced at all,
the player can also play well again opponents that slowly change their
way of playing. Systems can also learn to set up multi-move traps for a
shortsighted opponent.</p>
<p>This example shows how value function methods can use the structure
of the reinforcement learning: value functions can give credit to
independent moves, whereas evolutionary methods rewards the entire
policy, including moves that never occurred.</p>
<p>Reinforcement learning is applicable even in scenarios without an
explicit adversary and is not confined to problems with discrete time
steps or distinct episodes. It can effectively handle situations where
part of the state is hidden.</p>
<p>Reinforcement learning methods can be applied without a need for a
model, but models can be easily used if they are available. Model-free
approaches do not predict how the environment will react to an action,
yet they can offer advantages over more complex models, particularly due
to the challenges involved in creating an accurate model.</p>
<p>Reinforcement learning can also be used at higher levels. Although
the tic-tac-toe player only learned the basic moves of the game,
reinforcement learning can also work at higher levels where each
“action” may itself be the application of a problem-solving method.</p>
</body>
</html>
