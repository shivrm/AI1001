<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Shivram S ai24btech11031@iith.ac.in" />
  <title>AI1001 - Assignment 7</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">AI1001 - Assignment 7</h1>
<p class="author">Shivram S<br />
<code>ai24btech11031@iith.ac.in</code></p>
</header>
<p>An agent consists of an agent program which implements the agent
function, and a computing device with sensors and actuators, called the
agent architecture. The architecture makes percepts available to the
program, and feeds the program’s action choices to the actuators.</p>
<p>Agent programs can be classified into several types. Table-driven
agents use a table of all possible percent sequences to decide which
action to take. However, these tables can be huge - even simple games
like chess require a table of at least
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>10</mn><mn>150</mn></msup><annotation encoding="application/x-tex">10^{150}</annotation></semantics></math>
entries.</p>
<h1 id="simple-reflex-agents">Simple Reflex Agents</h1>
<p>The simplest kinds of agents are <strong>simple reflex
agents</strong>, which select actions based on the current percept,
ignoring the rest of the percept history. They are generally used in
simpler environments, but can occur even in more complex ones. These
agents work only if the environment is fully observable.</p>
<p>Simple reflex agents first generate an abstracted description of the
current state from the percept, then use condition-action rules
(<strong>if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle condition \rangle</annotation></semantics></math>
then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle action \rangle</annotation></semantics></math></strong>)
to decide which action to take. Sometimes, simple reflex agents fall
into infinite loops, especially in partially observable environments.
Escape from infinite loops is possible if the agent can randomize its
actions.</p>
<h1 id="model-based-reflex-agents">Model-based Reflex Agents</h1>
<p>Agents can handle partial observability by maintaining some sort of
<strong>internal state</strong> that depends on percept history. This
internal state information needs to updated as time goes on, which
requires information about how the world evolves independently of the
agent, and how the agent’s actions affect the world.</p>
<p>The knowledge about “how the world works” is called the
<strong>transition model</strong> of the world. The information about
how the state of the world is reflected in the agent’s percepts is
called a <strong>sensor model</strong>. The transition model and the
sensor model together allow an agent to keep track of the state of the
world. An agent that uses such models is called a <strong>model-based
agent</strong>.</p>
<p>A model-based reflex agent combines the current percept with the old
internal state to generate a description of the current state. The
action of the agent is then determined using rules.</p>
<h1 id="goal-based-agents">Goal-based Agents</h1>
<p>In order to determine which situations are desirable, an agent needs
some information about it’s <strong>goal</strong>. The agent program can
combine this with the model to choose actions that achieve the goal.
<strong>Search</strong> and <strong>Planning</strong> are subfields of
AI devoted to finding action sequences that achieve the agent’s
goals.</p>
<p>Goal-based decision making is fundamentally different from
condition-action rules in that it involves consideration of the future.
Goal-based agents are more flexible because the knowledge that supports
its decisions is represented explicitly and can be modified.</p>
<h1 id="utility-based-agents">Utility-based Agents</h1>
<p>A goal is just a crude binary-distinction (“achieved” vs. “not
achieved”). <strong>Utility</strong> is a more general performance
measure that allows a comparison of different world states. An agent’s
<strong>utility function</strong> is essentially an internalization of
the performance measure. If an agent’s performance measure and utility
function are aligned, then actions that maximize its utility will be
rational.</p>
<p>When it comes to flexibility and learning, a utility-based agent has
more advantages than a goal-based agent. Additionally, goal-based agents
can’t adequately handle cases where there are conflicting goals and
tradeoffs have to be made, or where no goals can be achieved with
certainty.</p>
<p>In a partially observable or nondeterministic environment, the agent
has to make decisions under uncertainty. A rational agent would maximize
the <strong>expected utility</strong> of the action outcome.</p>
<p>Building a utility-based agent is not easy. The agent has to model
and keep track of its environment. Choosing a utility-maximizing course
of action is also difficult and requires ingenious algorithms. Even with
these algorithms, perfect rationality is usually unobtainable due to
computational complexity.</p>
<h1 id="learning-agents">Learning Agents</h1>
<p>Agent programs can be created by “teaching” machines that can learn.
Due to the amount of work needed to program agents by hand, learning is
now the preferred method for creating state-of-the-art systems.</p>
<p>Learning allows an agent to operate in unknown environments and
become more competent over time. A learning agent consists of a
<strong>learning element</strong>, which is responsible for making
improvements and the <strong>performance element</strong>, which is
responsible for selecting external actions. The learning agent uses
feedback from a <strong>critic</strong> to determine how the performance
element should be modified. A <strong>problem generator</strong> is used
to suggest actions that could lead to new and informative
experiences.</p>
<p>The agent may learn directly from its percept sequence. For example,
successive states of the environment might allow the agent to learn
about what its actions do and how the world evolves in response to its
actions. An external performance standard is also needed to learn a
reflex component or utility function. The performance standard might
distinguish part of the percept as a <strong>reward</strong> or a
<strong>penalty</strong>.</p>
<h1 id="how-the-components-work">How the Components Work</h1>
<p>The state of the world can be represented in several ways, each with
different <strong>expressiveness</strong>. A more expressive
representation is more concise, but more complex to reason about. In an
<strong>atomic representation</strong>, the world is indivisible. Atomic
representations are used in search, game-playing, hidden Markov models
and Markov decision processes</p>
<p>In a <strong>factored representation</strong> , the state can be
split into a set of variables or attributes, each of which have a value.
Two factored states can share some values, making it much easier to Work
with turning one state to another. Factored representations are used in
constraint satisfaction, propositional logic, planning, Bayesian
networks and machine learning algorithms.</p>
<p>In a <strong>structured representation</strong>, objects and their
varying relationships can be described explicitly. Structured
representations are used in relational databases, first-order logic, and
much of natural language understanding.</p>
<p>The mapping of concepts to locations in physical memory can also be
divided into two. In a <strong>localist representation</strong>, there
is a one-to-one mapping between concepts and memory-locations. However,
in a <strong>distributed representation</strong>, the representation of
a concept is spread over many memory locations. Distributed
representations are more robust against noise and information loss.</p>
</body>
</html>
