<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>AI1001 - Assignment 9</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="assignment9.tex"> 
<link rel="stylesheet" type="text/css" href="style.css"> 
</head><body 
>
<div class="maketitle">_______________________________________________________________________________________________________________________________________________________________

<h2 class="titleHead">AI1001 - Assignment 9</h2>_____________________________________________________
          <div class="author" ><span 
class="ptmb7t-">Shivram S</span><br /><span 
class="cmtt-10">ai24btech11031@iith.ac.in</span></div>
<br />

       <hr class="float"><div class="float" 
>

       <span 
class="ptmr7t-x-x-90">Preprint. Under review.</span>

       </div><hr class="endfloat" />
       </div>
       <!--l. 13--><p class="noindent" >Our goal in machine learning is to select a hypothesis that will optimally fit future examples.
       We make the assumption that our future examples will be like the past, and follow the same
       probability distribution (<span 
class="ptmb7t-">stationarity </span>assumption), and that every example is independent of previous
       examples. Examples that satisfy these equations are independent and identically distributed
       (i.i.d.).
       <div class="eqnarray">
       <div class="math-display" >
       <img 
src="assignment90x.png" alt="P(Ej) = E (Ej+1) = P (Ej+2 ) = ...
  P (Ej) = P (Ej | Ej-1,Ej-2,...)
       " class="math-display" ></div>
       </div>
       <!--l. 25--><p class="noindent" >We can say that the optimal hypothesis is the one that minimizes the <span 
class="ptmb7t-">error rate</span>: the proportion of times that
       <span 
class="cmmi-10">h</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">x</span><span 
class="cmr-10">)</span><span 
class="cmmi-10">&#x2260;</span><span 
class="cmmi-10">y </span>for an <span 
class="cmr-10">(</span><span 
class="cmmi-10">x,y</span><span 
class="cmr-10">)</span>. This is done by measuring the model&#8217;s performance on a <span 
class="ptmb7t-">test set </span>of examples. To
       prevent the model from peeking at the test answers, we split the examples we have into a <span 
class="ptmb7t-">training set </span>and a
       <span 
class="ptmb7t-">test set</span>.
       <!--l. 32--><p class="noindent" >Our model class might have some parameters (called <span 
class="ptmb7t-">hyperparameters</span>) and we might want to
       find the optimal values for them. Tuning the hyperparameters by measuring error rates on the
       test set is also considered as peeking. So, we can divide our data data into a <span 
class="ptmb7t-">training set </span>to
       train the model, a <span 
class="ptmb7t-">validation set </span>to tune the hyperparameters, and a <span 
class="ptmb7t-">test set </span>to do an unbiased
       evaluation.
       <!--l. 39--><p class="noindent" >If we don&#8217;t have enough data, we can use <span 
class="ptmb7t-">k-fold cross validation</span>. We split the data into <span 
class="cmmi-10">k </span>equal subsets. We
       perform <span 
class="cmmi-10">k </span>rounds of learning. On each round, we use <span 
class="cmr-10">1</span><span 
class="cmmi-10">&#x2215;k </span>of the data as a validation set and the rest as the
       training set. The average test score of the <span 
class="cmmi-10">k </span>rounds is used. Popular values of <span 
class="cmmi-10">k </span>are 5 and 10, enough to give
       a better estimate at a cost of 5 to 10 times longer computation time. When <span 
class="cmmi-10">k </span><span 
class="cmr-10">= </span><span 
class="cmmi-10">n </span>we call it <span 
class="ptmb7t-">leave-one-out</span>
       <span 
class="ptmb7t-">cross validation </span>(LOOCV).
       <h3 class="sectionHead"><span class="titlemark">1    </span> <a 
 id="x1-10001"></a>Model Selection</h3>
       <!--l. 49--><p class="noindent" >Part of model selection is qualitative and subjective. Based on what we know about the problem, we might
       prefer some model classes over others. Then we can qualitatively select the best model class based on
       performance on the validation data set.
       <!--l. 54--><p class="noindent" >We can measure the complexity of a model based on attributes such as the number of nodes in a
       decision tree or the number of neural network parameters. The training set error approaches zero
       as the complexity increases, but the validation error starts to increase after some point due to
       overfitting. Some model classes, such as decision trees, never recover from overfitting. Others
       classes such as deep neural networks, kernel machines, etc. can use the larger capacity fit a larger
       number of suitable representations, hence the validation error tends to decrease as the capacity
       increases.
       <!--l. 64--><p class="noindent" >Model classes start to overfit as the capacity approaches the point of interpolation, which is when the model
       exactly fits all the training data. This is because the model&#8217;s capacity is concentrated on the training
       examples and the remaining capacity is allocated in a way that is not representative of the training

       data.
       <!--l. 70--><p class="noindent" >
       <h3 class="sectionHead"><span class="titlemark">2    </span> <a 
 id="x1-20002"></a>Loss Functions</h3>
       <!--l. 72--><p class="noindent" >In machine learning, it is traditional to express the model&#8217;s performance in terms of a <span 
class="ptmb7t-">loss function </span>that
       needs to be minimized. The loss function <span 
class="cmmi-10">L</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">x,y,</span><span 
class="cmmi-10">&#x0177;</span><span 
class="cmr-10">) </span>is the amount of utility lost by predicting <span 
class="cmmi-10">h</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">x</span><span 
class="cmr-10">) =</span> <span 
class="cmmi-10">&#x0177;</span>
       when the correct answer is <span 
class="cmmi-10">f</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">x</span><span 
class="cmr-10">) = </span><span 
class="cmmi-10">y</span>. We can often use a simplified version <span 
class="cmmi-10">L</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">y,</span><span 
class="cmmi-10">&#x0177;</span><span 
class="cmr-10">) </span>that is independent of
       <span 
class="cmmi-10">x</span>.
       <!--l. 79--><p class="noindent" >
       <div class="math-display" >
       <img 
src="assignment91x.png" alt="L (x,y,&#x02C6;y) = U tility(resultofusingy given x)- Utility(resultofusing &#x02C6;y given x)
       " class="math-display" ></div>
       <!--l. 84--><p class="noindent" >One misclassification might be worse than another. For example, if a spam-detection algorithm classifies a
       spam email as non-spam, then it&#8217;s just a minor annoyance, but if a non-spam email is classified as spam,
       then the user might miss an important message. Hence, we might want to give a larger value to
       <span 
class="cmmi-10">L</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">nospam,spam</span><span 
class="cmr-10">) </span>than to <span 
class="cmmi-10">L</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">spam,nospam</span><span 
class="cmr-10">)</span>.
       <!--l. 91--><p class="noindent" >We consider smaller errors to be better than larger ones. We can quantititatively implement this using loss
       functions such as <span 
class="cmmi-10">L</span><sub><span 
class="cmr-7">1</span></sub> loss and <span 
class="cmmi-10">L</span><sub><span 
class="cmr-7">2</span></sub> loss. For discrete-valued inputs, we can use the <span 
class="cmmi-10">L</span><sub><span 
class="cmr-7">0</span><span 
class="cmmi-7">&#x2215;</span><span 
class="cmr-7">1</span></sub> loss
       function.
       <!--l. 96--><p class="noindent" >
       <table 
class="align-star">
                                                       <tr><td 
class="align-odd"><span 
class="cmmi-10">L</span><sub><span 
class="cmr-7">1</span></sub><span 
class="cmr-10">(</span><span 
class="cmmi-10">y,</span><span 
class="cmmi-10">&#x0177;</span><span 
class="cmr-10">)</span></td>                                            <td 
class="align-even"> <span 
class="cmr-10">= </span><span 
class="cmsy-10">|</span><span 
class="cmmi-10">y </span><span 
class="cmsy-10">-</span><span 
class="cmmi-10">&#x0177;</span><span 
class="cmsy-10">|</span></td>                       <td 
class="align-label"></td>                  <td 
class="align-label">
                  </td></tr><tr><td 
class="align-odd"><span 
class="cmmi-10">L</span><sub><span 
class="cmr-7">2</span></sub><span 
class="cmr-10">(</span><span 
class="cmmi-10">y,</span><span 
class="cmmi-10">&#x0177;</span><span 
class="cmr-10">)</span></td>                                            <td 
class="align-even"> <span 
class="cmr-10">= (</span><span 
class="cmmi-10">y </span><span 
class="cmsy-10">-</span><span 
class="cmmi-10">&#x0177;</span><span 
class="cmr-10">)</span><sup><span 
class="cmr-7">2</span></sup></td>                                                               <td 
class="align-label"></td>                                                   <td 
class="align-label">
                                                   </td></tr><tr><td 
class="align-odd"><span 
class="cmmi-10">L</span><sub><span 
class="cmr-7">0</span><span 
class="cmmi-7">&#x2215;</span><span 
class="cmr-7">1</span></sub><span 
class="cmr-10">(</span><span 
class="cmmi-10">y,</span><span 
class="cmmi-10">&#x0177;</span><span 
class="cmr-10">)</span></td>                                         <td 
class="align-even"> <span 
class="cmr-10">=</span> 0 if <span 
class="cmmi-10">y </span><span 
class="cmr-10">=</span> <span 
class="cmmi-10">&#x0177;</span> else 1</td>                                                       <td 
class="align-label"></td>                                                       <td 
class="align-label"></td></tr></table>
       <!--l. 102--><p class="noindent" >To compute the expected loss over all input-output pairs, we define a probability distribution <span 
class="cmmi-10">P</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">X,Y </span><span 
class="cmr-10">) </span>over

       the examples. Then we can define the expected <span 
class="ptmb7t-">generalization loss </span>over the set of examples <span 
class="cmsy-10"><img 
src="cmsy10-45.png" alt="E" class="10x-x-45" /> </span>for a
       hypothesis <span 
class="cmmi-10">h </span>as:
       <!--l. 107--><p class="noindent" >
       <div class="math-display" >
       <img 
src="assignment92x.png" alt="               &#x2211;
GenLossL (h) =      L (y,h(x))&#x22C5;P (x,y)
              (x,y)&#x2208;E
       " class="math-display" ></div>
       <!--l. 111--><p class="noindent" >The best hypothesis <span 
class="cmmi-10">h</span><sup><span 
class="cmsy-7">*</span></sup> is the one that minimizes expected generalization loss:
       <div class="math-display" >
       <img 
src="assignment93x.png" alt="h* = argminGenLossL (h)
     h&#x2208;H
       " class="math-display" ></div>
       <!--l. 117--><p class="noindent" >Since <span 
class="cmmi-10">P</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">x,y</span><span 
class="cmr-10">) </span>is not known in most cases, the learning agent can only estimate generalization loss with
       <span 
class="ptmb7t-">empirical loss </span>on a set of examples <span 
class="cmmi-10">E </span>of size <span 
class="cmmi-10">N</span>.
       <!--l. 121--><p class="noindent" >
       <div class="math-display" >
       <img 
src="assignment94x.png" alt="                  &#x2211;            1-
EmpLossL,E (h) =      L(y,h(x))N
                (x,y)&#x2208;E
       " class="math-display" ></div>
       <!--l. 125--><p class="noindent" >The estimated best hypothesis <span 
class="cmmi-10">&#x0125;</span><sup><span 
class="cmsy-7">*</span></sup> is the one with the minimum empirical loss:
       <div class="math-display" >
       <img 
src="assignment95x.png" alt=" *
&#x02C6;h  = arghm&#x2208;iHn EmpLossL,E (h ).
       " class="math-display" ></div>
       <!--l. 131--><p class="noindent" ><span 
class="cmmi-10">&#x0125;</span><sup><span 
class="cmsy-7">*</span></sup> may differ from the true function <span 
class="cmmi-10">f </span>due to unrealizability, variance, noise and computational complexity.
       A problem is realiizable if the hypothesis space <span 
class="cmsy-10"><img 
src="cmsy10-48.png" alt="H" class="10x-x-48" /> </span>actually contains the true function <span 
class="cmmi-10">f</span>. <span 
class="cmmi-10">f </span>may
       be non-deterministic or noisy - it may return different values of <span 
class="cmmi-10">f</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">x</span><span 
class="cmr-10">) </span>for the same value of
       <span 
class="cmmi-10">x</span>.
       <!--l. 137--><p class="noindent" >The early years of machine learning concentrated on <span 
class="ptmb7t-">small-scale learning </span>where the number of training
       example ranges from dozens to the low thousands, and generalization loss usually came from
       approximation error of not having <span 
class="cmmi-10">f </span>in the hypothesis space. Recently there has been a shift towards
       <span 
class="ptmb7t-">large-scale learning </span>with millions of examples, where the generalization loss is dominated by limits of
       computation.
       <!--l. 144--><p class="noindent" >
       <h3 class="sectionHead"><span class="titlemark">3    </span> <a 
 id="x1-30003"></a>Regularization</h3>
       <!--l. 146--><p class="noindent" >Complicated hypotheses have a tendency to overfit. <span 
class="ptmb7t-">Regularization </span>is the practice of penalizing complex
       hypotheses. This is done using a <span 
class="ptmb7t-">regularization function </span>which depends on the hypothesis space. For
       example, for polynomials, a choice of regularization function may be the sum of squares of coefficients.
       Taking regularization into account, we can define the total cost as:
       <!--l. 152--><p class="noindent" >
       <div class="math-display" >
       <img 
src="assignment96x.png" alt="Cost(h) = EmpLoss (h) + &#x03BB;&#x00D7; Complexity(h)
       " class="math-display" ></div>
       <!--l. 156--><p class="noindent" ><span 
class="cmmi-10">&#x03BB; </span>is a hyperparameter that serves as a conversion rate between loss and complexity. However, it is possible
       to avoid the conversion factor by encoding the hypothesis as a Turing machine program and counting the
       number of bits required to encode the data. The <span 
class="ptmb7t-">minimum description length </span>hypothesis minimizes the
       total number of bits required.
       <!--l. 162--><p class="noindent" >Models can also be simplified by reducing the dimensions that they work with. <span 
class="ptmb7t-">Feature selection </span>can be
       performed to discard irrelevant attributes.
       <!--l. 165--><p class="noindent" >
       <h3 class="sectionHead"><span class="titlemark">4    </span> <a 
 id="x1-40004"></a>Hyperparameter Tuning</h3>
       <!--l. 167--><p class="noindent" >We also want to select the best values for hyperparameters. The simplest approach is <span 
class="ptmb7t-">hand-tuning</span>, where
       we guess parameter values based on past experience or intuition. If there are only a small number of
       possible values, <span 
class="ptmb7t-">grid-search </span>can be used, which tries all combinations and sees which performs best on the
       validation data. If there are too many combinations, then we might use <span 
class="ptmb7t-">random search </span>by
       sampling some values randomly and repating as long as we are willing to spend the time and
       resources.

       <!--l. 175--><p class="noindent" ><span 
class="ptmb7t-">Bayesian Optimization </span>treats hyperparameter tuning as a machine learning problem in itself. We think of
       the vector of hyperparameters, <span 
class="ptmb7t-">x</span> as an input, and try to find the function <span 
class="cmmi-10">y </span><span 
class="cmr-10">= </span><span 
class="cmmi-10">f</span><span 
class="cmr-10">(</span><span 
class="ptmb7t-">x</span><span 
class="cmr-10">) </span>which minimizes the loss
       <span 
class="cmmi-10">y</span>. Each pair of <span 
class="cmr-10">(</span><span 
class="cmmi-10">y,f</span><span 
class="cmr-10">(</span><span 
class="ptmb7t-">x</span><span 
class="cmr-10">)) </span>can be used to update our belief about the shape of the function <span 
class="cmmi-10">f</span>. We want to
       trade off exploitation (choosing parameter values near a previous good result) with exploration (trying novel
       values).
       <!--l. 182--><p class="noindent" >An alternative to Bayesian optimization is <span 
class="ptmb7t-">population-based-training </span>(PBT). PBT first uses random search
       to train a population of models, each with different hyperparameter values, then training a second
       generation whose hyperparameter values are determined by the best-performing values of the previous
       generation, plus random mutation.
        
</body></html> 



